# ğŸ§  Intelligent Order Document Parser â€“ Project Architecture

This project simulates a lightweight version of Endeavor AIâ€™s order processing agents by ingesting order documents (PDFs, scanned images, emails), extracting structured data, and outputting ERP-ready JSON.

---

## ğŸ“ Project Structure

order-parser/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ routes.py
â”‚ â”‚ â”œâ”€â”€ parser.py
â”‚ â”‚ â”œâ”€â”€ ocr.py
â”‚ â”‚ â”œâ”€â”€ ner_model.py
â”‚ â”‚ â””â”€â”€ utils.py
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ sample_docs/
â”‚ â”‚ â””â”€â”€ ground_truth/
â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â””â”€â”€ ner_bert_model/
â”‚ â”œâ”€â”€ tests/
â”‚ â”‚ â””â”€â”€ test_parser.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ run.py
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ public/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”œâ”€â”€ pages/
â”‚ â”‚ â”œâ”€â”€ App.jsx
â”‚ â”‚ â””â”€â”€ index.js
â”‚ â”œâ”€â”€ tailwind.config.js
â”‚ â””â”€â”€ vite.config.js
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ docs/
â”œâ”€â”€ architecture.md
â””â”€â”€ demo_screenshots/

yaml
Copy
Edit

---

## ğŸ”§ Component Responsibilities

### `backend/`

#### `run.py`
- Flask app entry point
- Initializes API and sets up routes

#### `routes.py`
- Defines endpoints:
  - `/upload`: handle file uploads
  - `/parse`: call the parser pipeline

#### `ocr.py`
- Uses `pytesseract` or `pdfplumber` to extract raw text from:
  - Scanned images
  - Native PDFs
- Handles filetype routing (PDF â†’ direct text | JPG â†’ OCR)

#### `parser.py`
- Cleans raw text using regex and `spaCy`
- Applies rules or calls NER model to extract fields
- Outputs structured dictionary of:
  - Customer info
  - Order ID
  - Line items (SKU, quantity, price)
  - Shipping address

#### `ner_model.py`
- Optional: loads BERT model from `transformers`
- Fine-tunes on labeled entity data if ML mode is used

#### `utils.py`
- Helper functions for:
  - JSON export
  - Field confidence estimation
  - Address normalization

#### `data/sample_docs/`
- Holds sample PDFs, PNGs, or `.txt` files

#### `data/ground_truth/`
- Annotated correct outputs for evaluation

#### `models/`
- Trained BERT or rule templates

#### `tests/`
- Unit tests for parser and OCR

---

### `frontend/` (Optional)

#### `App.jsx`
- Main component: drag-and-drop uploader
- Preview of:
  - Raw extracted text
  - Parsed output (JSON table view)

#### `components/`
- File uploader, result viewer, loading spinner

#### `pages/`
- Upload page
- Output preview page

#### API Calls
- POST to `/upload`
- GET from `/parse`

---

## ğŸ”„ How Services Connect

```plaintext
User Upload (Frontend)
   â†“
[React UI]  â†’  `/upload` (Flask API)
                   â†“
              OCR Engine (`ocr.py`)
                   â†“
         Cleaned Text â†’ `parser.py`
                   â†“
     â†³ Rule-based or ML NER (`ner_model.py`)
                   â†“
      Structured Output (dict/JSON)
                   â†“
               `/parse` â†’ Frontend
                   â†“
     Table + JSON View in React UI
ğŸŒ API Endpoints (Flask)
Route	Method	Purpose
/upload	POST	Accept file, return raw text
/parse	GET	Return extracted order fields (JSON)

ğŸ§ª Evaluation (Optional)
Use ground_truth/ + predicted outputs

Evaluate:

Entity Precision, Recall, F1 (via seqeval)

Order completeness (% fields correctly extracted)

ğŸš€ Deployment Suggestions
Part	Platform
Frontend	Vercel
Backend	Render / Railway
Model	Self-hosted or HuggingFace Hub
OCR	Dockerize Tesseract if needed

ğŸ“Œ Notes
You can disable the frontend if needed and demo with Postman or curl.

Start with rule-based parsing, then upgrade to ML-based NER.

Use Label Studio or doccano if you want to label your own training data.